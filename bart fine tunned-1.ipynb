{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "83ed7e8e",
   "metadata": {},
   "source": [
    "# BART-Base Fine-tuning for Document Summarization\n",
    "This notebook demonstrates fine-tuning BART-base model on CNN/DailyMail dataset for document summarization tasks."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "95abd6fb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting evaluate\n",
      "  Using cached evaluate-0.4.5-py3-none-any.whl.metadata (9.5 kB)\n",
      "Collecting accelerate\n",
      "  Using cached accelerate-1.10.1-py3-none-any.whl.metadata (19 kB)\n",
      "Requirement already satisfied: datasets>=2.0.0 in c:\\users\\gowth\\.conda\\envs\\summarizer\\lib\\site-packages (from evaluate) (4.0.0)\n",
      "Requirement already satisfied: numpy>=1.17 in c:\\users\\gowth\\.conda\\envs\\summarizer\\lib\\site-packages (from evaluate) (2.1.2)\n",
      "Requirement already satisfied: dill in c:\\users\\gowth\\.conda\\envs\\summarizer\\lib\\site-packages (from evaluate) (0.3.8)\n",
      "Requirement already satisfied: pandas in c:\\users\\gowth\\.conda\\envs\\summarizer\\lib\\site-packages (from evaluate) (2.3.2)\n",
      "Requirement already satisfied: requests>=2.19.0 in c:\\users\\gowth\\.conda\\envs\\summarizer\\lib\\site-packages (from evaluate) (2.32.5)\n",
      "Requirement already satisfied: tqdm>=4.62.1 in c:\\users\\gowth\\.conda\\envs\\summarizer\\lib\\site-packages (from evaluate) (4.67.1)\n",
      "Requirement already satisfied: xxhash in c:\\users\\gowth\\.conda\\envs\\summarizer\\lib\\site-packages (from evaluate) (3.5.0)\n",
      "Requirement already satisfied: multiprocess in c:\\users\\gowth\\.conda\\envs\\summarizer\\lib\\site-packages (from evaluate) (0.70.16)\n",
      "Requirement already satisfied: fsspec>=2021.05.0 in c:\\users\\gowth\\.conda\\envs\\summarizer\\lib\\site-packages (from fsspec[http]>=2021.05.0->evaluate) (2024.6.1)\n",
      "Requirement already satisfied: huggingface-hub>=0.7.0 in c:\\users\\gowth\\.conda\\envs\\summarizer\\lib\\site-packages (from evaluate) (0.34.4)\n",
      "Requirement already satisfied: packaging in c:\\users\\gowth\\.conda\\envs\\summarizer\\lib\\site-packages (from evaluate) (25.0)\n",
      "Requirement already satisfied: psutil in c:\\users\\gowth\\.conda\\envs\\summarizer\\lib\\site-packages (from accelerate) (7.0.0)\n",
      "Requirement already satisfied: pyyaml in c:\\users\\gowth\\.conda\\envs\\summarizer\\lib\\site-packages (from accelerate) (6.0.2)\n",
      "Requirement already satisfied: torch>=2.0.0 in c:\\users\\gowth\\.conda\\envs\\summarizer\\lib\\site-packages (from accelerate) (2.5.1+cu121)\n",
      "Requirement already satisfied: safetensors>=0.4.3 in c:\\users\\gowth\\.conda\\envs\\summarizer\\lib\\site-packages (from accelerate) (0.6.2)\n",
      "Requirement already satisfied: filelock in c:\\users\\gowth\\.conda\\envs\\summarizer\\lib\\site-packages (from datasets>=2.0.0->evaluate) (3.13.1)\n",
      "Requirement already satisfied: pyarrow>=15.0.0 in c:\\users\\gowth\\.conda\\envs\\summarizer\\lib\\site-packages (from datasets>=2.0.0->evaluate) (21.0.0)\n",
      "Requirement already satisfied: aiohttp!=4.0.0a0,!=4.0.0a1 in c:\\users\\gowth\\.conda\\envs\\summarizer\\lib\\site-packages (from fsspec[http]>=2021.05.0->evaluate) (3.12.15)\n",
      "Requirement already satisfied: aiohappyeyeballs>=2.5.0 in c:\\users\\gowth\\.conda\\envs\\summarizer\\lib\\site-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]>=2021.05.0->evaluate) (2.6.1)\n",
      "Requirement already satisfied: aiosignal>=1.4.0 in c:\\users\\gowth\\.conda\\envs\\summarizer\\lib\\site-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]>=2021.05.0->evaluate) (1.4.0)\n",
      "Requirement already satisfied: async-timeout<6.0,>=4.0 in c:\\users\\gowth\\.conda\\envs\\summarizer\\lib\\site-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]>=2021.05.0->evaluate) (5.0.1)\n",
      "Requirement already satisfied: attrs>=17.3.0 in c:\\users\\gowth\\.conda\\envs\\summarizer\\lib\\site-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]>=2021.05.0->evaluate) (25.3.0)\n",
      "Requirement already satisfied: frozenlist>=1.1.1 in c:\\users\\gowth\\.conda\\envs\\summarizer\\lib\\site-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]>=2021.05.0->evaluate) (1.7.0)\n",
      "Requirement already satisfied: multidict<7.0,>=4.5 in c:\\users\\gowth\\.conda\\envs\\summarizer\\lib\\site-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]>=2021.05.0->evaluate) (6.6.4)\n",
      "Requirement already satisfied: propcache>=0.2.0 in c:\\users\\gowth\\.conda\\envs\\summarizer\\lib\\site-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]>=2021.05.0->evaluate) (0.3.2)\n",
      "Requirement already satisfied: yarl<2.0,>=1.17.0 in c:\\users\\gowth\\.conda\\envs\\summarizer\\lib\\site-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]>=2021.05.0->evaluate) (1.20.1)\n",
      "Requirement already satisfied: typing-extensions>=4.1.0 in c:\\users\\gowth\\.conda\\envs\\summarizer\\lib\\site-packages (from multidict<7.0,>=4.5->aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]>=2021.05.0->evaluate) (4.12.2)\n",
      "Requirement already satisfied: idna>=2.0 in c:\\users\\gowth\\.conda\\envs\\summarizer\\lib\\site-packages (from yarl<2.0,>=1.17.0->aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]>=2021.05.0->evaluate) (3.10)\n",
      "Requirement already satisfied: charset_normalizer<4,>=2 in c:\\users\\gowth\\.conda\\envs\\summarizer\\lib\\site-packages (from requests>=2.19.0->evaluate) (3.4.3)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in c:\\users\\gowth\\.conda\\envs\\summarizer\\lib\\site-packages (from requests>=2.19.0->evaluate) (2.5.0)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in c:\\users\\gowth\\.conda\\envs\\summarizer\\lib\\site-packages (from requests>=2.19.0->evaluate) (2025.8.3)\n",
      "Requirement already satisfied: networkx in c:\\users\\gowth\\.conda\\envs\\summarizer\\lib\\site-packages (from torch>=2.0.0->accelerate) (3.3)\n",
      "Requirement already satisfied: jinja2 in c:\\users\\gowth\\.conda\\envs\\summarizer\\lib\\site-packages (from torch>=2.0.0->accelerate) (3.1.4)\n",
      "Requirement already satisfied: sympy==1.13.1 in c:\\users\\gowth\\.conda\\envs\\summarizer\\lib\\site-packages (from torch>=2.0.0->accelerate) (1.13.1)\n",
      "Requirement already satisfied: mpmath<1.4,>=1.1.0 in c:\\users\\gowth\\.conda\\envs\\summarizer\\lib\\site-packages (from sympy==1.13.1->torch>=2.0.0->accelerate) (1.3.0)\n",
      "Requirement already satisfied: colorama in c:\\users\\gowth\\.conda\\envs\\summarizer\\lib\\site-packages (from tqdm>=4.62.1->evaluate) (0.4.6)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in c:\\users\\gowth\\.conda\\envs\\summarizer\\lib\\site-packages (from jinja2->torch>=2.0.0->accelerate) (2.1.5)\n",
      "Requirement already satisfied: python-dateutil>=2.8.2 in c:\\users\\gowth\\.conda\\envs\\summarizer\\lib\\site-packages (from pandas->evaluate) (2.9.0.post0)\n",
      "Requirement already satisfied: pytz>=2020.1 in c:\\users\\gowth\\.conda\\envs\\summarizer\\lib\\site-packages (from pandas->evaluate) (2025.2)\n",
      "Requirement already satisfied: tzdata>=2022.7 in c:\\users\\gowth\\.conda\\envs\\summarizer\\lib\\site-packages (from pandas->evaluate) (2025.2)\n",
      "Requirement already satisfied: six>=1.5 in c:\\users\\gowth\\.conda\\envs\\summarizer\\lib\\site-packages (from python-dateutil>=2.8.2->pandas->evaluate) (1.17.0)\n",
      "Using cached evaluate-0.4.5-py3-none-any.whl (84 kB)\n",
      "Using cached accelerate-1.10.1-py3-none-any.whl (374 kB)\n",
      "Installing collected packages: accelerate, evaluate\n",
      "\n",
      "   ---------------------------------------- 0/2 [accelerate]\n",
      "   ---------------------------------------- 0/2 [accelerate]\n",
      "   ---------------------------------------- 0/2 [accelerate]\n",
      "   ---------------------------------------- 0/2 [accelerate]\n",
      "   ---------------------------------------- 0/2 [accelerate]\n",
      "   ---------------------------------------- 0/2 [accelerate]\n",
      "   ---------------------------------------- 0/2 [accelerate]\n",
      "   ---------------------------------------- 0/2 [accelerate]\n",
      "   ---------------------------------------- 0/2 [accelerate]\n",
      "   ---------------------------------------- 0/2 [accelerate]\n",
      "   ---------------------------------------- 0/2 [accelerate]\n",
      "   ---------------------------------------- 0/2 [accelerate]\n",
      "   ---------------------------------------- 0/2 [accelerate]\n",
      "   ---------------------------------------- 0/2 [accelerate]\n",
      "   ---------------------------------------- 0/2 [accelerate]\n",
      "   ---------------------------------------- 0/2 [accelerate]\n",
      "   ---------------------------------------- 0/2 [accelerate]\n",
      "   ---------------------------------------- 0/2 [accelerate]\n",
      "   ---------------------------------------- 0/2 [accelerate]\n",
      "   ---------------------------------------- 0/2 [accelerate]\n",
      "   ---------------------------------------- 0/2 [accelerate]\n",
      "   ---------------------------------------- 0/2 [accelerate]\n",
      "   ---------------------------------------- 0/2 [accelerate]\n",
      "   ---------------------------------------- 0/2 [accelerate]\n",
      "   -------------------- ------------------- 1/2 [evaluate]\n",
      "   -------------------- ------------------- 1/2 [evaluate]\n",
      "   -------------------- ------------------- 1/2 [evaluate]\n",
      "   -------------------- ------------------- 1/2 [evaluate]\n",
      "   -------------------- ------------------- 1/2 [evaluate]\n",
      "   -------------------- ------------------- 1/2 [evaluate]\n",
      "   -------------------- ------------------- 1/2 [evaluate]\n",
      "   ---------------------------------------- 2/2 [evaluate]\n",
      "\n",
      "Successfully installed accelerate-1.10.1 evaluate-0.4.5\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "# Install required packages\n",
    "%pip install evaluate accelerate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "674de093",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: cuda\n",
      "GPU: NVIDIA GeForce RTX 3050 Laptop GPU\n",
      "GPU Memory: 4.0 GB\n"
     ]
    }
   ],
   "source": [
    "# Import libraries\n",
    "import torch\n",
    "from datasets import load_dataset\n",
    "from transformers import (\n",
    "    AutoTokenizer, \n",
    "    AutoModelForSeq2SeqLM, \n",
    "    DataCollatorForSeq2Seq,\n",
    "    TrainingArguments,\n",
    "    Trainer,\n",
    "    pipeline\n",
    ")\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# Check device\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "print(f\"Using device: {device}\")\n",
    "if torch.cuda.is_available():\n",
    "    print(f\"GPU: {torch.cuda.get_device_name(0)}\")\n",
    "    print(f\"GPU Memory: {torch.cuda.get_device_properties(0).total_memory / 1024**3:.1f} GB\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "30324f3c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading CNN/DailyMail dataset...\n",
      "Training samples: 5000\n",
      "Validation samples: 500\n",
      "\n",
      "Sample article (first 200 chars):\n",
      "LONDON, England (Reuters) -- Harry Potter star Daniel Radcliffe gains access to a reported £20 million ($41.1 million) fortune as he turns 18 on Monday, but he insists the money won't cast a spell on ...\n",
      "\n",
      "Sample summary:\n",
      "Harry Potter star Daniel Radcliffe gets £20M fortune as he turns 18 Monday .\n",
      "Young actor says he has no plans to fritter his cash away .\n",
      "Radcliffe's earnings from first five Potter films have been held in trust fund .\n"
     ]
    }
   ],
   "source": [
    "# Load CNN/DailyMail dataset\n",
    "print(\"Loading CNN/DailyMail dataset...\")\n",
    "dataset = load_dataset('abisee/cnn_dailymail', '3.0.0')\n",
    "\n",
    "# Use subset for faster training - adjust as needed\n",
    "train_subset_size = 5000  # Use 5k samples for training\n",
    "val_subset_size = 500     # Use 500 samples for validation\n",
    "\n",
    "dataset['train'] = dataset['train'].select(range(train_subset_size))\n",
    "dataset['validation'] = dataset['validation'].select(range(val_subset_size))\n",
    "\n",
    "print(f\"Training samples: {len(dataset['train'])}\")\n",
    "print(f\"Validation samples: {len(dataset['validation'])}\")\n",
    "\n",
    "# Show a sample\n",
    "print(\"\\nSample article (first 200 chars):\")\n",
    "print(dataset['train'][0]['article'][:200] + \"...\")\n",
    "print(\"\\nSample summary:\")\n",
    "print(dataset['train'][0]['highlights'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "5dae34fc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading facebook/bart-base...\n",
      "Model loaded: facebook/bart-base\n",
      "Model parameters: 139,420,416\n",
      "Tokenizer vocab size: 50265\n",
      "Model moved to cuda\n"
     ]
    }
   ],
   "source": [
    "# Load BART-base model and tokenizer\n",
    "model_name = \"facebook/bart-base\"\n",
    "print(f\"Loading {model_name}...\")\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "model = AutoModelForSeq2SeqLM.from_pretrained(model_name)\n",
    "\n",
    "print(f\"Model loaded: {model_name}\")\n",
    "print(f\"Model parameters: {model.num_parameters():,}\")\n",
    "print(f\"Tokenizer vocab size: {tokenizer.vocab_size}\")\n",
    "\n",
    "# Move model to device\n",
    "model.to(device)\n",
    "print(f\"Model moved to {device}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "e4653d13",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Preprocessing dataset...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "35b83b0904fc4a3998eda2c37ea03f33",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/11490 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Preprocessing completed!\n",
      "Tokenized training samples: 5000\n",
      "Tokenized validation samples: 500\n"
     ]
    }
   ],
   "source": [
    "# Preprocessing function\n",
    "def preprocess_function(examples):\n",
    "    # Get articles and summaries\n",
    "    articles = [doc for doc in examples[\"article\"]]\n",
    "    summaries = [doc for doc in examples[\"highlights\"]]\n",
    "    \n",
    "    # Tokenize inputs and targets\n",
    "    model_inputs = tokenizer(\n",
    "        articles,\n",
    "        max_length=512,          # Max input length\n",
    "        truncation=True,\n",
    "        padding=True,\n",
    "        return_tensors=\"pt\"\n",
    "    )\n",
    "    \n",
    "    # Tokenize targets (summaries)\n",
    "    labels = tokenizer(\n",
    "        text_target=summaries,\n",
    "        max_length=128,          # Max summary length\n",
    "        truncation=True,\n",
    "        padding=True,\n",
    "        return_tensors=\"pt\"\n",
    "    )\n",
    "    \n",
    "    model_inputs[\"labels\"] = labels[\"input_ids\"]\n",
    "    return model_inputs\n",
    "\n",
    "# Apply preprocessing to dataset\n",
    "print(\"Preprocessing dataset...\")\n",
    "tokenized_dataset = dataset.map(\n",
    "    preprocess_function,\n",
    "    batched=True,\n",
    "    remove_columns=dataset[\"train\"].column_names\n",
    ")\n",
    "\n",
    "print(\"Preprocessing completed!\")\n",
    "print(f\"Tokenized training samples: {len(tokenized_dataset['train'])}\")\n",
    "print(f\"Tokenized validation samples: {len(tokenized_dataset['validation'])}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c67d9403",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Preprocessing function\n",
    "def preprocess_function(examples):\n",
    "    # Get articles and summaries\n",
    "    articles = [doc for doc in examples[\"article\"]]\n",
    "    summaries = [doc for doc in examples[\"highlights\"]]\n",
    "    \n",
    "    # Tokenize inputs and targets\n",
    "    model_inputs = tokenizer(\n",
    "        articles,\n",
    "        max_length=512,          # Max input length\n",
    "        truncation=True,\n",
    "        padding=True,\n",
    "        return_tensors=\"pt\"\n",
    "    )\n",
    "    \n",
    "    # Tokenize targets (summaries)\n",
    "    labels = tokenizer(\n",
    "        text_target=summaries,\n",
    "        max_length=128,          # Max summary length\n",
    "        truncation=True,\n",
    "        padding=True,\n",
    "        return_tensors=\"pt\"\n",
    "    )\n",
    "    \n",
    "    model_inputs[\"labels\"] = labels[\"input_ids\"]\n",
    "    return model_inputs\n",
    "\n",
    "# Apply preprocessing to dataset\n",
    "print(\"Preprocessing dataset...\")\n",
    "tokenized_dataset = dataset.map(\n",
    "    preprocess_function,\n",
    "    batched=True,\n",
    "    remove_columns=dataset[\"train\"].column_names\n",
    ")\n",
    "\n",
    "print(\"Preprocessing completed!\")\n",
    "print(f\"Tokenized training samples: {len(tokenized_dataset['train'])}\")\n",
    "print(f\"Tokenized validation samples: {len(tokenized_dataset['validation'])}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "f6eb17c0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training arguments configured:\n",
      "  - Epochs: 2\n",
      "  - Batch size: 4\n",
      "  - Learning rate: 3e-05\n",
      "  - Mixed precision: True\n"
     ]
    }
   ],
   "source": [
    "# Data collator for dynamic padding\n",
    "data_collator = DataCollatorForSeq2Seq(\n",
    "    tokenizer=tokenizer,\n",
    "    model=model,\n",
    "    padding=True\n",
    ")\n",
    "\n",
    "# Training arguments optimized for speed\n",
    "training_args = TrainingArguments(\n",
    "    output_dir=\"./bart_cnn_summarization\",\n",
    "    num_train_epochs=2,                    # 2 epochs for balance of speed/quality\n",
    "    per_device_train_batch_size=4,         # Adjust based on your GPU memory\n",
    "    per_device_eval_batch_size=4,\n",
    "    gradient_accumulation_steps=2,         # Effective batch size = 4*2 = 8\n",
    "    learning_rate=3e-5,                    # Good learning rate for fine-tuning\n",
    "    weight_decay=0.01,\n",
    "    warmup_steps=500,\n",
    "    logging_steps=100,\n",
    "    eval_strategy=\"steps\",\n",
    "    eval_steps=500,\n",
    "    save_steps=1000,\n",
    "    save_total_limit=2,\n",
    "    load_best_model_at_end=True,\n",
    "    metric_for_best_model=\"eval_loss\",\n",
    "    greater_is_better=False,\n",
    "    fp16=True,                             # Mixed precision for speed\n",
    "    dataloader_num_workers=2,\n",
    "    remove_unused_columns=True,\n",
    "    report_to=\"none\",                      # Disable wandb\n",
    "    push_to_hub=False,\n",
    ")\n",
    "\n",
    "print(\"Training arguments configured:\")\n",
    "print(f\"  - Epochs: {training_args.num_train_epochs}\")\n",
    "print(f\"  - Batch size: {training_args.per_device_train_batch_size}\")\n",
    "print(f\"  - Learning rate: {training_args.learning_rate}\")\n",
    "print(f\"  - Mixed precision: {training_args.fp16}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "0852b386",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Trainer initialized!\n",
      "Starting training...\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='1250' max='1250' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [1250/1250 1:33:01, Epoch 2/2]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Step</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>500</td>\n",
       "      <td>1.584400</td>\n",
       "      <td>1.170038</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1000</td>\n",
       "      <td>1.353100</td>\n",
       "      <td>1.144550</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "There were missing keys in the checkpoint model loaded: ['model.encoder.embed_tokens.weight', 'model.decoder.embed_tokens.weight', 'lm_head.weight'].\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training completed successfully!\n"
     ]
    }
   ],
   "source": [
    "# Initialize Trainer\n",
    "trainer = Trainer(\n",
    "    model=model,\n",
    "    args=training_args,\n",
    "    train_dataset=tokenized_dataset[\"train\"],\n",
    "    eval_dataset=tokenized_dataset[\"validation\"],\n",
    "    data_collator=data_collator,\n",
    "    tokenizer=tokenizer,\n",
    ")\n",
    "\n",
    "print(\"Trainer initialized!\")\n",
    "print(\"Starting training...\")\n",
    "\n",
    "# Start training\n",
    "try:\n",
    "    trainer.train()\n",
    "    print(\"Training completed successfully!\")\n",
    "except Exception as e:\n",
    "    print(f\"Training failed with error: {e}\")\n",
    "    raise"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "06ec76a3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model saved to: ./fine_tuned_bart_cnn\n",
      "You can now use this model for inference!\n"
     ]
    }
   ],
   "source": [
    "# Save the fine-tuned model\n",
    "model_save_path = \"./fine_tuned_bart_cnn\"\n",
    "trainer.save_model(model_save_path)\n",
    "tokenizer.save_pretrained(model_save_path)\n",
    "\n",
    "print(f\"Model saved to: {model_save_path}\")\n",
    "print(\"You can now use this model for inference!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "956f4142",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "summarizer",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
