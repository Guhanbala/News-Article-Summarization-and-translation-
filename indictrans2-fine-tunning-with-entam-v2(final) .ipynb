{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_cell_guid": "b1076dfc-b9ad-4769-8c92-a6c4dae69d19",
    "_uuid": "8f2839f25d086af736a60e9eeb907d3b93b6e0e5",
    "execution": {
     "iopub.execute_input": "2025-10-10T22:35:58.679253Z",
     "iopub.status.busy": "2025-10-10T22:35:58.678763Z",
     "iopub.status.idle": "2025-10-10T22:36:02.383281Z",
     "shell.execute_reply": "2025-10-10T22:36:02.382371Z",
     "shell.execute_reply.started": "2025-10-10T22:35:58.679216Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "# CELL 1: Install Dependencies\n",
    "%pip install transformers==4.44.2 datasets==3.0.1 sentencepiece sacrebleu torch accelerate pandas tqdm evaluate rouge-score IndicTransToolkit -q\n",
    "print(\"‚úÖ Installation complete!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-10-10T22:36:02.384916Z",
     "iopub.status.busy": "2025-10-10T22:36:02.384657Z",
     "iopub.status.idle": "2025-10-10T22:36:10.241643Z",
     "shell.execute_reply": "2025-10-10T22:36:10.240906Z",
     "shell.execute_reply.started": "2025-10-10T22:36:02.384893Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üñ•Ô∏è Using device: cuda\n",
      "GPU: NVIDIA GeForce RTX 3050 Laptop GPU\n",
      "Memory: 4.29 GB\n"
     ]
    }
   ],
   "source": [
    "# CELL 2: Imports and Device Setup\n",
    "import torch\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from datasets import load_dataset, Dataset, DatasetDict\n",
    "from transformers import (\n",
    "    Seq2SeqTrainingArguments,\n",
    "    Seq2SeqTrainer,\n",
    "    DataCollatorForSeq2Seq,\n",
    "    EarlyStoppingCallback\n",
    ")\n",
    "import evaluate\n",
    "from tqdm.auto import tqdm\n",
    "import warnings\n",
    "import os\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "print(f\"üñ•Ô∏è Using device: {device}\")\n",
    "if torch.cuda.is_available():\n",
    "    print(f\"GPU: {torch.cuda.get_device_name(0)}\")\n",
    "    print(f\"Memory: {torch.cuda.get_device_properties(0).total_memory / 1e9:.2f} GB\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-10-10T22:36:10.243179Z",
     "iopub.status.busy": "2025-10-10T22:36:10.242600Z",
     "iopub.status.idle": "2025-10-10T22:36:10.249347Z",
     "shell.execute_reply": "2025-10-10T22:36:10.248471Z",
     "shell.execute_reply.started": "2025-10-10T22:36:10.243151Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ Configuration loaded!\n",
      "üìä Training: eng_Latn ‚Üí tam_Taml\n"
     ]
    }
   ],
   "source": [
    "# CELL 3: Configuration\n",
    "import torch\n",
    "class Config:\n",
    "    MODEL_NAME = \"ai4bharat/indictrans2-en-indic-dist-200M\"\n",
    "    SOURCE_LANG = \"eng_Latn\"  # English (Latin script)\n",
    "    TARGET_LANG = \"tam_Taml\"  # Tamil (Tamil script)\n",
    "    SOURCE_CODE = \"en\"\n",
    "    TARGET_CODE = \"ta\"\n",
    "    MAX_INPUT_LENGTH = 64  # Increased for longer sentences\n",
    "    MAX_TARGET_LENGTH = 64  # Increased for longer sentences\n",
    "    BATCH_SIZE = 3\n",
    "    LEARNING_RATE = 3e-5  # Slightly higher for faster adaptation\n",
    "    NUM_EPOCHS = 1  # More epochs\n",
    "    WARMUP_STEPS = 500  # Adjusted for larger data\n",
    "    WEIGHT_DECAY = 0.01\n",
    "    NUM_BEAMS = 2  # Increased for better generation\n",
    "    OUTPUT_DIR = \"./indictrans2-finetuned-news-translation\"\n",
    "    LOGGING_DIR = \"./logs\"\n",
    "    SEED = 42\n",
    "    FP16 = torch.cuda.is_available()\n",
    "    SAVE_STEPS = 250 # Eval more often\n",
    "    EVAL_STEPS = 250\n",
    "    LOGGING_STEPS = 100\n",
    "    GRAD_ACCUM_STEPS = 4 # For effective larger batch\n",
    "\n",
    "config = Config()\n",
    "print(\"‚úÖ Configuration loaded!\")\n",
    "print(f\"üìä Training: {config.SOURCE_LANG} ‚Üí {config.TARGET_LANG}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-10-10T22:36:10.251002Z",
     "iopub.status.busy": "2025-10-10T22:36:10.250797Z",
     "iopub.status.idle": "2025-10-10T22:36:11.863004Z",
     "shell.execute_reply": "2025-10-10T22:36:11.862259Z",
     "shell.execute_reply.started": "2025-10-10T22:36:10.250985Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üì• Loading EnTam v2 dataset from Kaggle path...\n",
      "\n",
      "Contents of directory (D:\\Work\\Projects\\S5\\TA\\en-ta-parallel-v2\\en-ta-parallel-v2):\n",
      "['corpus.bcn.dev.en', 'corpus.bcn.dev.ta', 'corpus.bcn.test.en', 'corpus.bcn.test.ta', 'corpus.bcn.train.en', 'corpus.bcn.train.ta']\n",
      "\n",
      "Original full train examples: 166871\n",
      "\n",
      "‚úÖ Dataset loaded and subsetted!\n",
      "Train examples: 5000\n",
      "Validation examples: 500\n",
      "Test examples: 500\n",
      "\n",
      "üìù Sample data:\n",
      "English (src): The Centre Party also claimed that Estonian 'independence' was being sacrificed by joining the EU....\n",
      "Tamil (tgt): ‡Æê‡Æ∞‡Øá‡Ææ‡Æ™‡Øç‡Æ™‡Æø‡ÆØ ‡Æí‡Æ©‡Øç‡Æ±‡Æø‡ÆØ‡Æ§‡Øç‡Æ§‡Æø‡Æ≤‡Øç ‡Æá‡Æ£‡Øà‡Æµ‡Æ§‡Æ©‡Øç ‡ÆÆ‡ØÇ‡Æ≤‡ÆÆ‡Øç ‡Æé‡Æ∏‡Øç‡Æ§‡Øá‡Ææ‡Æ©‡Æø‡ÆØ ‡Æ®‡Ææ‡Æü‡Øç‡Æü‡Æø‡Æ©‡Øç \"‡Æö‡ØÅ‡Æ§‡Æ®‡Øç‡Æ§‡Æø‡Æ∞‡ÆÆ‡Øç\" ‡Æ™‡Æ±‡Æø‡Æ™‡Øá‡Ææ‡ÆØ‡Øç‡Æµ‡Æø‡Æü‡ØÅ‡ÆÆ‡Øç ‡Æé‡Æ©‡Øç‡Æ±‡ØÅ ‡ÆÆ‡Æ§‡Øç‡Æ§‡Æø‡ÆØ ‡Æï‡Æü‡Øç...\n"
     ]
    }
   ],
   "source": [
    "# CELL 4: Load EnTam v2 Dataset from Kaggle Path\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from datasets import Dataset, DatasetDict\n",
    "import random\n",
    "import os\n",
    "import torch\n",
    "\n",
    "SEED = 42\n",
    "random.seed(SEED)\n",
    "torch.manual_seed(SEED)\n",
    "np.random.seed(SEED)\n",
    "\n",
    "print(\"üì• Loading EnTam v2 dataset from Kaggle path...\")\n",
    "extracted_path = \"D:\\Work\\Projects\\S5\\TA\\en-ta-parallel-v2\\en-ta-parallel-v2\"\n",
    "print(f\"\\nContents of directory ({extracted_path}):\")\n",
    "if os.path.exists(extracted_path):\n",
    "    print(os.listdir(extracted_path))\n",
    "else:\n",
    "    raise Exception(f\"Directory not found: {extracted_path}\")\n",
    "\n",
    "try:\n",
    "    with open(os.path.join(extracted_path, \"corpus.bcn.train.en\"), 'r', encoding='utf-8') as en_f, \\\n",
    "         open(os.path.join(extracted_path, \"corpus.bcn.train.ta\"), 'r', encoding='utf-8') as ta_f:\n",
    "        en_lines = [line.strip() for line in en_f.readlines() if line.strip()]\n",
    "        ta_lines = [line.strip() for line in ta_f.readlines() if line.strip()]\n",
    "    min_length = min(len(en_lines), len(ta_lines))\n",
    "    df = pd.DataFrame({'src': en_lines[:min_length], 'tgt': ta_lines[:min_length]})\n",
    "except FileNotFoundError:\n",
    "    raise Exception(f\"Dataset files corpus.bcn.train.en or corpus.bcn.train.ta not found in {extracted_path}\")\n",
    "except Exception as e:\n",
    "    raise Exception(f\"Error loading dataset files: {e}\")\n",
    "\n",
    "full_dataset = Dataset.from_pandas(df)\n",
    "print(f\"\\nOriginal full train examples: {len(full_dataset)}\")\n",
    "\n",
    "train_test = full_dataset.train_test_split(test_size=0.2, seed=SEED)\n",
    "train_dataset_full = train_test['train']\n",
    "val_test = train_test['test'].train_test_split(test_size=0.5, seed=SEED)\n",
    "val_dataset_full = val_test['train']\n",
    "test_dataset_full = val_test['test']\n",
    "\n",
    "train_size = 5000 # Scaled up\n",
    "val_size = 500\n",
    "test_size = 500\n",
    "\n",
    "train_dataset = train_dataset_full.shuffle(seed=SEED).select(range(min(train_size, len(train_dataset_full))))\n",
    "val_dataset = val_dataset_full.shuffle(seed=SEED).select(range(min(val_size, len(val_dataset_full))))\n",
    "test_dataset = test_dataset_full.shuffle(seed=SEED).select(range(min(test_size, len(test_dataset_full))))\n",
    "\n",
    "raw_datasets = DatasetDict({\n",
    "    'train': train_dataset,\n",
    "    'validation': val_dataset,\n",
    "    'test': test_dataset\n",
    "})\n",
    "\n",
    "print(f\"\\n‚úÖ Dataset loaded and subsetted!\")\n",
    "print(f\"Train examples: {len(raw_datasets['train'])}\")\n",
    "print(f\"Validation examples: {len(raw_datasets['validation'])}\")\n",
    "print(f\"Test examples: {len(raw_datasets['test'])}\")\n",
    "\n",
    "print(\"\\nüìù Sample data:\")\n",
    "sample = raw_datasets['train'][0]\n",
    "print(f\"English (src): {sample['src'][:100]}...\")\n",
    "print(f\"Tamil (tgt): {sample['tgt'][:100]}...\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-10-10T22:36:11.863962Z",
     "iopub.status.busy": "2025-10-10T22:36:11.863733Z",
     "iopub.status.idle": "2025-10-10T22:36:14.650545Z",
     "shell.execute_reply": "2025-10-10T22:36:14.649832Z",
     "shell.execute_reply.started": "2025-10-10T22:36:11.863944Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: IndicTransToolkit in c:\\users\\gowth\\.conda\\envs\\torchenv\\lib\\site-packages (1.1.1)\n",
      "Requirement already satisfied: cython in c:\\users\\gowth\\.conda\\envs\\torchenv\\lib\\site-packages (from IndicTransToolkit) (3.1.4)\n",
      "Requirement already satisfied: sacremoses in c:\\users\\gowth\\.conda\\envs\\torchenv\\lib\\site-packages (from IndicTransToolkit) (0.1.1)\n",
      "Requirement already satisfied: transformers in c:\\users\\gowth\\.conda\\envs\\torchenv\\lib\\site-packages (from IndicTransToolkit) (4.44.2)\n",
      "Requirement already satisfied: sacrebleu in c:\\users\\gowth\\.conda\\envs\\torchenv\\lib\\site-packages (from IndicTransToolkit) (2.5.1)\n",
      "Requirement already satisfied: indic-nlp-library-itt in c:\\users\\gowth\\.conda\\envs\\torchenv\\lib\\site-packages (from IndicTransToolkit) (0.1.1)\n",
      "Requirement already satisfied: morfessor in c:\\users\\gowth\\.conda\\envs\\torchenv\\lib\\site-packages (from indic-nlp-library-itt->IndicTransToolkit) (2.0.6)\n",
      "Requirement already satisfied: numpy in c:\\users\\gowth\\.conda\\envs\\torchenv\\lib\\site-packages (from indic-nlp-library-itt->IndicTransToolkit) (2.0.1)\n",
      "Requirement already satisfied: pandas in c:\\users\\gowth\\.conda\\envs\\torchenv\\lib\\site-packages (from indic-nlp-library-itt->IndicTransToolkit) (2.3.2)\n",
      "Requirement already satisfied: sphinx-argparse in c:\\users\\gowth\\.conda\\envs\\torchenv\\lib\\site-packages (from indic-nlp-library-itt->IndicTransToolkit) (0.5.2)\n",
      "Requirement already satisfied: sphinx-rtd-theme in c:\\users\\gowth\\.conda\\envs\\torchenv\\lib\\site-packages (from indic-nlp-library-itt->IndicTransToolkit) (3.0.2)\n",
      "Requirement already satisfied: python-dateutil>=2.8.2 in c:\\users\\gowth\\.conda\\envs\\torchenv\\lib\\site-packages (from pandas->indic-nlp-library-itt->IndicTransToolkit) (2.9.0.post0)\n",
      "Requirement already satisfied: pytz>=2020.1 in c:\\users\\gowth\\.conda\\envs\\torchenv\\lib\\site-packages (from pandas->indic-nlp-library-itt->IndicTransToolkit) (2025.2)\n",
      "Requirement already satisfied: tzdata>=2022.7 in c:\\users\\gowth\\.conda\\envs\\torchenv\\lib\\site-packages (from pandas->indic-nlp-library-itt->IndicTransToolkit) (2025.2)\n",
      "Requirement already satisfied: six>=1.5 in c:\\users\\gowth\\.conda\\envs\\torchenv\\lib\\site-packages (from python-dateutil>=2.8.2->pandas->indic-nlp-library-itt->IndicTransToolkit) (1.17.0)\n",
      "Requirement already satisfied: portalocker in c:\\users\\gowth\\.conda\\envs\\torchenv\\lib\\site-packages (from sacrebleu->IndicTransToolkit) (3.2.0)\n",
      "Requirement already satisfied: regex in c:\\users\\gowth\\.conda\\envs\\torchenv\\lib\\site-packages (from sacrebleu->IndicTransToolkit) (2025.9.1)\n",
      "Requirement already satisfied: tabulate>=0.8.9 in c:\\users\\gowth\\.conda\\envs\\torchenv\\lib\\site-packages (from sacrebleu->IndicTransToolkit) (0.9.0)\n",
      "Requirement already satisfied: colorama in c:\\users\\gowth\\.conda\\envs\\torchenv\\lib\\site-packages (from sacrebleu->IndicTransToolkit) (0.4.6)\n",
      "Requirement already satisfied: lxml in c:\\users\\gowth\\.conda\\envs\\torchenv\\lib\\site-packages (from sacrebleu->IndicTransToolkit) (6.0.2)\n",
      "Requirement already satisfied: pywin32>=226 in c:\\users\\gowth\\.conda\\envs\\torchenv\\lib\\site-packages (from portalocker->sacrebleu->IndicTransToolkit) (311)\n",
      "Requirement already satisfied: click in c:\\users\\gowth\\.conda\\envs\\torchenv\\lib\\site-packages (from sacremoses->IndicTransToolkit) (8.3.0)\n",
      "Requirement already satisfied: joblib in c:\\users\\gowth\\.conda\\envs\\torchenv\\lib\\site-packages (from sacremoses->IndicTransToolkit) (1.5.2)\n",
      "Requirement already satisfied: tqdm in c:\\users\\gowth\\.conda\\envs\\torchenv\\lib\\site-packages (from sacremoses->IndicTransToolkit) (4.67.1)\n",
      "Requirement already satisfied: sphinx>=5.1.0 in c:\\users\\gowth\\.conda\\envs\\torchenv\\lib\\site-packages (from sphinx-argparse->indic-nlp-library-itt->IndicTransToolkit) (8.1.3)\n",
      "Requirement already satisfied: docutils>=0.19 in c:\\users\\gowth\\.conda\\envs\\torchenv\\lib\\site-packages (from sphinx-argparse->indic-nlp-library-itt->IndicTransToolkit) (0.21.2)\n",
      "Requirement already satisfied: sphinxcontrib-applehelp>=1.0.7 in c:\\users\\gowth\\.conda\\envs\\torchenv\\lib\\site-packages (from sphinx>=5.1.0->sphinx-argparse->indic-nlp-library-itt->IndicTransToolkit) (2.0.0)\n",
      "Requirement already satisfied: sphinxcontrib-devhelp>=1.0.6 in c:\\users\\gowth\\.conda\\envs\\torchenv\\lib\\site-packages (from sphinx>=5.1.0->sphinx-argparse->indic-nlp-library-itt->IndicTransToolkit) (2.0.0)\n",
      "Requirement already satisfied: sphinxcontrib-htmlhelp>=2.0.6 in c:\\users\\gowth\\.conda\\envs\\torchenv\\lib\\site-packages (from sphinx>=5.1.0->sphinx-argparse->indic-nlp-library-itt->IndicTransToolkit) (2.1.0)\n",
      "Requirement already satisfied: sphinxcontrib-jsmath>=1.0.1 in c:\\users\\gowth\\.conda\\envs\\torchenv\\lib\\site-packages (from sphinx>=5.1.0->sphinx-argparse->indic-nlp-library-itt->IndicTransToolkit) (1.0.1)\n",
      "Requirement already satisfied: sphinxcontrib-qthelp>=1.0.6 in c:\\users\\gowth\\.conda\\envs\\torchenv\\lib\\site-packages (from sphinx>=5.1.0->sphinx-argparse->indic-nlp-library-itt->IndicTransToolkit) (2.0.0)\n",
      "Requirement already satisfied: sphinxcontrib-serializinghtml>=1.1.9 in c:\\users\\gowth\\.conda\\envs\\torchenv\\lib\\site-packages (from sphinx>=5.1.0->sphinx-argparse->indic-nlp-library-itt->IndicTransToolkit) (2.0.0)\n",
      "Requirement already satisfied: Jinja2>=3.1 in c:\\users\\gowth\\.conda\\envs\\torchenv\\lib\\site-packages (from sphinx>=5.1.0->sphinx-argparse->indic-nlp-library-itt->IndicTransToolkit) (3.1.6)\n",
      "Requirement already satisfied: Pygments>=2.17 in c:\\users\\gowth\\.conda\\envs\\torchenv\\lib\\site-packages (from sphinx>=5.1.0->sphinx-argparse->indic-nlp-library-itt->IndicTransToolkit) (2.19.2)\n",
      "Requirement already satisfied: snowballstemmer>=2.2 in c:\\users\\gowth\\.conda\\envs\\torchenv\\lib\\site-packages (from sphinx>=5.1.0->sphinx-argparse->indic-nlp-library-itt->IndicTransToolkit) (3.0.1)\n",
      "Requirement already satisfied: babel>=2.13 in c:\\users\\gowth\\.conda\\envs\\torchenv\\lib\\site-packages (from sphinx>=5.1.0->sphinx-argparse->indic-nlp-library-itt->IndicTransToolkit) (2.17.0)\n",
      "Requirement already satisfied: alabaster>=0.7.14 in c:\\users\\gowth\\.conda\\envs\\torchenv\\lib\\site-packages (from sphinx>=5.1.0->sphinx-argparse->indic-nlp-library-itt->IndicTransToolkit) (1.0.0)\n",
      "Requirement already satisfied: imagesize>=1.3 in c:\\users\\gowth\\.conda\\envs\\torchenv\\lib\\site-packages (from sphinx>=5.1.0->sphinx-argparse->indic-nlp-library-itt->IndicTransToolkit) (1.4.1)\n",
      "Requirement already satisfied: requests>=2.30.0 in c:\\users\\gowth\\.conda\\envs\\torchenv\\lib\\site-packages (from sphinx>=5.1.0->sphinx-argparse->indic-nlp-library-itt->IndicTransToolkit) (2.32.5)\n",
      "Requirement already satisfied: packaging>=23.0 in c:\\users\\gowth\\.conda\\envs\\torchenv\\lib\\site-packages (from sphinx>=5.1.0->sphinx-argparse->indic-nlp-library-itt->IndicTransToolkit) (25.0)\n",
      "Requirement already satisfied: tomli>=2 in c:\\users\\gowth\\.conda\\envs\\torchenv\\lib\\site-packages (from sphinx>=5.1.0->sphinx-argparse->indic-nlp-library-itt->IndicTransToolkit) (2.2.1)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in c:\\users\\gowth\\.conda\\envs\\torchenv\\lib\\site-packages (from Jinja2>=3.1->sphinx>=5.1.0->sphinx-argparse->indic-nlp-library-itt->IndicTransToolkit) (3.0.2)\n",
      "Requirement already satisfied: charset_normalizer<4,>=2 in c:\\users\\gowth\\.conda\\envs\\torchenv\\lib\\site-packages (from requests>=2.30.0->sphinx>=5.1.0->sphinx-argparse->indic-nlp-library-itt->IndicTransToolkit) (3.3.2)\n",
      "Requirement already satisfied: idna<4,>=2.5 in c:\\users\\gowth\\.conda\\envs\\torchenv\\lib\\site-packages (from requests>=2.30.0->sphinx>=5.1.0->sphinx-argparse->indic-nlp-library-itt->IndicTransToolkit) (3.7)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in c:\\users\\gowth\\.conda\\envs\\torchenv\\lib\\site-packages (from requests>=2.30.0->sphinx>=5.1.0->sphinx-argparse->indic-nlp-library-itt->IndicTransToolkit) (2.5.0)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in c:\\users\\gowth\\.conda\\envs\\torchenv\\lib\\site-packages (from requests>=2.30.0->sphinx>=5.1.0->sphinx-argparse->indic-nlp-library-itt->IndicTransToolkit) (2025.8.3)\n",
      "Requirement already satisfied: sphinxcontrib-jquery<5,>=4 in c:\\users\\gowth\\.conda\\envs\\torchenv\\lib\\site-packages (from sphinx-rtd-theme->indic-nlp-library-itt->IndicTransToolkit) (4.1)\n",
      "Requirement already satisfied: filelock in c:\\users\\gowth\\.conda\\envs\\torchenv\\lib\\site-packages (from transformers->IndicTransToolkit) (3.17.0)\n",
      "Requirement already satisfied: huggingface-hub<1.0,>=0.23.2 in c:\\users\\gowth\\.conda\\envs\\torchenv\\lib\\site-packages (from transformers->IndicTransToolkit) (0.34.4)\n",
      "Requirement already satisfied: pyyaml>=5.1 in c:\\users\\gowth\\.conda\\envs\\torchenv\\lib\\site-packages (from transformers->IndicTransToolkit) (6.0.2)\n",
      "Requirement already satisfied: safetensors>=0.4.1 in c:\\users\\gowth\\.conda\\envs\\torchenv\\lib\\site-packages (from transformers->IndicTransToolkit) (0.6.2)\n",
      "Requirement already satisfied: tokenizers<0.20,>=0.19 in c:\\users\\gowth\\.conda\\envs\\torchenv\\lib\\site-packages (from transformers->IndicTransToolkit) (0.19.1)\n",
      "Requirement already satisfied: fsspec>=2023.5.0 in c:\\users\\gowth\\.conda\\envs\\torchenv\\lib\\site-packages (from huggingface-hub<1.0,>=0.23.2->transformers->IndicTransToolkit) (2024.6.1)\n",
      "Requirement already satisfied: typing-extensions>=3.7.4.3 in c:\\users\\gowth\\.conda\\envs\\torchenv\\lib\\site-packages (from huggingface-hub<1.0,>=0.23.2->transformers->IndicTransToolkit) (4.15.0)\n",
      "Note: you may need to restart the kernel to use updated packages.\n",
      "ü§ñ Loading IndicTrans2 model and tokenizer from Hugging Face...\n",
      "Using device: cuda\n",
      "‚úÖ Model loaded successfully!\n",
      "Model parameters: 211.78M\n"
     ]
    }
   ],
   "source": [
    "# CELL 5: Load Model and Tokenizer\n",
    "\n",
    "%pip install IndicTransToolkit\n",
    "\n",
    "from transformers import AutoModelForSeq2SeqLM, AutoTokenizer\n",
    "from IndicTransToolkit.processor import IndicProcessor\n",
    "import torch\n",
    "\n",
    "print(\"ü§ñ Loading IndicTrans2 model and tokenizer from Hugging Face...\")\n",
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "print(f\"Using device: {device}\")\n",
    "\n",
    "try:\n",
    "    # Load tokenizer and model directly from Hugging Face\n",
    "    tokenizer = AutoTokenizer.from_pretrained(\"ai4bharat/indictrans2-en-indic-dist-200M\", trust_remote_code=True)\n",
    "    model = AutoModelForSeq2SeqLM.from_pretrained(\"ai4bharat/indictrans2-en-indic-dist-200M\", trust_remote_code=True)\n",
    "    model = model.to(device)\n",
    "\n",
    "    # Initialize IndicProcessor without lang argument, handle language later if needed\n",
    "    processor = IndicProcessor(inference=True)  # Removed 'lang=config.TARGET_LANG'\n",
    "\n",
    "    print(f\"‚úÖ Model loaded successfully!\")\n",
    "    print(f\"Model parameters: {model.num_parameters() / 1e6:.2f}M\")\n",
    "\n",
    "except Exception as e:\n",
    "    print(f\"‚ùå Model loading failed: {e}\")\n",
    "    print(\"Ensure internet is enabled and IndicTransToolkit is installed. Try restarting the kernel.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-10-10T22:36:14.651571Z",
     "iopub.status.busy": "2025-10-10T22:36:14.651300Z",
     "iopub.status.idle": "2025-10-10T22:36:14.656974Z",
     "shell.execute_reply": "2025-10-10T22:36:14.656193Z",
     "shell.execute_reply.started": "2025-10-10T22:36:14.651545Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ Preprocessing function defined!\n"
     ]
    }
   ],
   "source": [
    "# CELL 6: Preprocessing Function\n",
    "def preprocess_function(examples):\n",
    "    inputs = [f\"{config.SOURCE_LANG} {config.TARGET_LANG} {src}\" for src in examples['src']]  # Add language tags\n",
    "    targets = examples['tgt']\n",
    "    \n",
    "    # Tokenize inputs\n",
    "    model_inputs = tokenizer(\n",
    "        inputs,\n",
    "        max_length=config.MAX_INPUT_LENGTH,\n",
    "        truncation=True,\n",
    "        padding=False\n",
    "    )\n",
    "    \n",
    "    # Tokenize targets\n",
    "    with tokenizer.as_target_tokenizer():\n",
    "        labels = tokenizer(\n",
    "            targets,\n",
    "            max_length=config.MAX_TARGET_LENGTH,\n",
    "            truncation=True,\n",
    "            padding=False\n",
    "        )\n",
    "    \n",
    "    model_inputs['labels'] = labels['input_ids']\n",
    "    return model_inputs\n",
    "\n",
    "print(\"‚úÖ Preprocessing function defined!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-10-10T22:36:14.657891Z",
     "iopub.status.busy": "2025-10-10T22:36:14.657659Z",
     "iopub.status.idle": "2025-10-10T22:36:21.051016Z",
     "shell.execute_reply": "2025-10-10T22:36:21.050378Z",
     "shell.execute_reply.started": "2025-10-10T22:36:14.657863Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üîÑ Tokenizing dataset...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9712171cad4e4595aa90668cebf1dfe7",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Tokenizing:   0%|          | 0/5000 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "45d8a40e958044bca605f2d99ff1cd62",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Tokenizing:   0%|          | 0/500 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "8f252152b1d9469cbb54b2790b72d8a7",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Tokenizing:   0%|          | 0/500 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ Tokenization complete!\n",
      "Train samples: 5000\n",
      "Validation samples: 500\n",
      "Test samples: 500\n"
     ]
    }
   ],
   "source": [
    "# CELL 7: Tokenize Dataset\n",
    "print(\"üîÑ Tokenizing dataset...\")\n",
    "\n",
    "tokenized_datasets = raw_datasets.map(\n",
    "    preprocess_function,\n",
    "    batched=True,\n",
    "    remove_columns=raw_datasets['train'].column_names,\n",
    "    desc=\"Tokenizing\"\n",
    ")\n",
    "\n",
    "print(\"‚úÖ Tokenization complete!\")\n",
    "print(f\"Train samples: {len(tokenized_datasets['train'])}\")\n",
    "print(f\"Validation samples: {len(tokenized_datasets['validation'])}\")\n",
    "print(f\"Test samples: {len(tokenized_datasets['test'])}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-10-10T22:36:21.052212Z",
     "iopub.status.busy": "2025-10-10T22:36:21.051862Z",
     "iopub.status.idle": "2025-10-10T22:36:25.159970Z",
     "shell.execute_reply": "2025-10-10T22:36:25.159091Z",
     "shell.execute_reply.started": "2025-10-10T22:36:21.052185Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ Evaluation metrics configured!\n"
     ]
    }
   ],
   "source": [
    "# CELL 8: Setup Evaluation Metrics\n",
    "import evaluate\n",
    "import numpy as np  ### CHANGED: Added explicit numpy import (was missing)\n",
    "from IndicTransToolkit.processor import IndicProcessor  # Ensure this is imported\n",
    "\n",
    "bleu_metric = evaluate.load('sacrebleu')\n",
    "rouge_metric = evaluate.load('rouge')  ### CHANGED: Added ROUGE and chrF loads\n",
    "chrf_metric = evaluate.load('chrf')\n",
    "\n",
    "# Reuse processor from CELL 5 (assume it's global; reinitialize if needed)\n",
    "# processor = IndicProcessor(inference=True)  ### CHANGED: Commented reinitialize option for safety\n",
    "\n",
    "def postprocess_text(preds, labels):\n",
    "    # Strip and normalize predictions\n",
    "    preds = [pred.strip() for pred in preds]\n",
    "    preds = processor.postprocess_batch(preds, lang=config.TARGET_LANG)  ### CHANGED: Swapped to correct batch method (was post_process_sentence)\n",
    "    \n",
    "    # Normalize references (labels) similarly for consistency\n",
    "    normalized_labels = []  ### CHANGED: New loop to batch-process labels (prevents single-item errors)\n",
    "    for label in labels:\n",
    "        norm_label = processor.postprocess_batch([label.strip()], lang=config.TARGET_LANG)[0]  ### CHANGED: Batch even single labels\n",
    "        normalized_labels.append(norm_label)\n",
    "    labels = [[lbl] for lbl in normalized_labels]  # SacreBLEU expects list of list(str)\n",
    "    \n",
    "    return preds, labels\n",
    "\n",
    "def compute_metrics(eval_preds):\n",
    "    preds, labels = eval_preds\n",
    "    if isinstance(preds, tuple):\n",
    "        preds = preds[0]\n",
    "    labels = np.where(labels != -100, labels, tokenizer.pad_token_id)  ### CHANGED: Now uses np (imported above)\n",
    "    \n",
    "    decoded_preds = tokenizer.batch_decode(preds, skip_special_tokens=True)\n",
    "    decoded_labels = tokenizer.batch_decode(labels, skip_special_tokens=True)\n",
    "    \n",
    "    decoded_preds, decoded_labels = postprocess_text(decoded_preds, decoded_labels)\n",
    "    \n",
    "    bleu_result = bleu_metric.compute(predictions=decoded_preds, references=decoded_labels)\n",
    "    rouge_result = rouge_metric.compute(predictions=decoded_preds, references=decoded_labels)  ### CHANGED: Added ROUGE/chrF computes\n",
    "    chrf_result = chrf_metric.compute(predictions=decoded_preds, references=decoded_labels)\n",
    "    \n",
    "    result = {\n",
    "        'bleu': bleu_result['score'],\n",
    "        'rouge1': rouge_result['rouge1'],  ### CHANGED: Added to result dict\n",
    "        'chrf': chrf_result['score']  ### CHANGED: Added to result dict\n",
    "    }\n",
    "    \n",
    "    result = {k: round(v, 4) for k, v in result.items()}\n",
    "    return result\n",
    "\n",
    "print(\"‚úÖ Evaluation metrics configured!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-10-10T22:37:25.493307Z",
     "iopub.status.busy": "2025-10-10T22:37:25.492959Z",
     "iopub.status.idle": "2025-10-10T22:37:28.783892Z",
     "shell.execute_reply": "2025-10-10T22:37:28.783097Z",
     "shell.execute_reply.started": "2025-10-10T22:37:25.493275Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ Training arguments configured!\n"
     ]
    }
   ],
   "source": [
    "\n",
    "from transformers import Seq2SeqTrainingArguments, Seq2SeqTrainer, DataCollatorForSeq2Seq\n",
    "import torch\n",
    "\n",
    "# Make sure device is defined\n",
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "\n",
    "# Optional: define output directory if config not available\n",
    "OUTPUT_DIR = getattr(config, \"OUTPUT_DIR\", \"./mt5-finetuned-news-translation\")\n",
    "\n",
    "training_args = Seq2SeqTrainingArguments(\n",
    "    output_dir=OUTPUT_DIR,\n",
    "    eval_strategy=\"steps\",          # use eval_strategy instead of evaluation_strategy\n",
    "    eval_steps=getattr(config, \"EVAL_STEPS\", 1000),\n",
    "    save_strategy=\"steps\",\n",
    "    save_steps=getattr(config, \"SAVE_STEPS\", 1000),\n",
    "    per_device_train_batch_size=getattr(config, \"BATCH_SIZE\", 4),\n",
    "    per_device_eval_batch_size=getattr(config, \"BATCH_SIZE\", 4),\n",
    "    gradient_accumulation_steps=getattr(config, \"GRAD_ACCUM_STEPS\", 4),  # New: larger effective batch\n",
    "    learning_rate=getattr(config, \"LEARNING_RATE\", 5e-5),\n",
    "    weight_decay=getattr(config, \"WEIGHT_DECAY\", 0.01),\n",
    "    save_total_limit=getattr(config, \"SAVE_TOTAL_LIMIT\", 3),\n",
    "    num_train_epochs=getattr(config, \"NUM_EPOCHS\", 1),\n",
    "    predict_with_generate=True,\n",
    "    generation_max_length=getattr(config, \"MAX_TARGET_LENGTH\", 128),\n",
    "    generation_num_beams=getattr(config, \"NUM_BEAMS\", 4),\n",
    "    fp16=getattr(config, \"FP16\", torch.cuda.is_available()),\n",
    "    logging_dir=getattr(config, \"LOGGING_DIR\", \"./logs\"),\n",
    "    load_best_model_at_end=True,\n",
    "    metric_for_best_model='bleu',\n",
    "    greater_is_better=True,\n",
    "    warmup_steps=getattr(config, \"WARMUP_STEPS\", 500),\n",
    "    #report_to=\"none\",\n",
    "    seed=getattr(config, \"SEED\", 42),\n",
    "    save_safetensors=False, # Added to fix non-contiguous tensor save error\n",
    "    logging_strategy=\"steps\",  # Log every X steps\n",
    "    logging_steps=50,  # Frequent (every 50 steps; adjust to 100 if too noisy)\n",
    "    report_to=[\"tensorboard\"],  # Enables live graphs (install !pip install tensorboard if needed)\n",
    ")\n",
    "\n",
    "# Data collator\n",
    "data_collator = DataCollatorForSeq2Seq(\n",
    "    tokenizer=tokenizer,\n",
    "    model=model,\n",
    "    padding=True,\n",
    "    return_tensors=\"pt\"\n",
    ")\n",
    "\n",
    "print(\"‚úÖ Training arguments configured!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-10-10T22:37:32.435585Z",
     "iopub.status.busy": "2025-10-10T22:37:32.434745Z",
     "iopub.status.idle": "2025-10-10T22:37:32.750494Z",
     "shell.execute_reply": "2025-10-10T22:37:32.749659Z",
     "shell.execute_reply.started": "2025-10-10T22:37:32.435552Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ Trainer initialized!\n",
      "\n",
      "üìä Training Configuration:\n",
      "  ‚Ä¢ Batch size: 3 (effective: 12)\n",
      "  ‚Ä¢ Learning rate: 3e-05\n",
      "  ‚Ä¢ Epochs: 1\n",
      "  ‚Ä¢ Total training steps: 1666\n"
     ]
    }
   ],
   "source": [
    "# CELL 10: Initialize Trainer\n",
    "from transformers import EarlyStoppingCallback\n",
    "trainer = Seq2SeqTrainer(\n",
    "    model=model,\n",
    "    args=training_args,\n",
    "    train_dataset=tokenized_datasets['train'],\n",
    "    eval_dataset=tokenized_datasets['validation'],\n",
    "    tokenizer=tokenizer,\n",
    "    data_collator=data_collator,\n",
    "    compute_metrics=compute_metrics,\n",
    "    callbacks=[EarlyStoppingCallback(early_stopping_patience=3)]  # Stop if no improvement\n",
    ")\n",
    "\n",
    "print(\"‚úÖ Trainer initialized!\")\n",
    "print(\"\\nüìä Training Configuration:\")\n",
    "print(f\"  ‚Ä¢ Batch size: {config.BATCH_SIZE} (effective: {config.BATCH_SIZE * 4})\")\n",
    "print(f\"  ‚Ä¢ Learning rate: {config.LEARNING_RATE}\")\n",
    "print(f\"  ‚Ä¢ Epochs: {config.NUM_EPOCHS}\")\n",
    "print(f\"  ‚Ä¢ Total training steps: {len(tokenized_datasets['train']) // config.BATCH_SIZE * config.NUM_EPOCHS}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-10-10T22:37:37.667068Z",
     "iopub.status.busy": "2025-10-10T22:37:37.666759Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üöÄ Starting training...\n",
      "======================================================================\n",
      "üßπ GPU cache cleared!\n",
      "Memory before training: 0.85 GB\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "40419ff1b1284cf8943f68d18f8d7c6e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/416 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 9.4834, 'grad_norm': 7.160135269165039, 'learning_rate': 3e-06, 'epoch': 0.12}\n",
      "{'loss': 7.7502, 'grad_norm': 5.7943115234375, 'learning_rate': 6e-06, 'epoch': 0.24}\n",
      "{'loss': 5.6016, 'grad_norm': 4.3690595626831055, 'learning_rate': 9e-06, 'epoch': 0.36}\n",
      "{'loss': 4.6176, 'grad_norm': 3.24072003364563, 'learning_rate': 1.2e-05, 'epoch': 0.48}\n",
      "{'loss': 4.0149, 'grad_norm': 2.606715440750122, 'learning_rate': 1.5e-05, 'epoch': 0.6}\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "cc7a27cd9c8f461b96c4372d3dbf3b09",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/167 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# CELL 11: Start Training\n",
    "os.environ[\"PYTORCH_CUDA_ALLOC_CONF\"] = \"expandable_segments:True\"\n",
    "\n",
    "print(\"üöÄ Starting training...\")\n",
    "print(\"=\"*70)\n",
    "\n",
    "try:\n",
    "    torch.cuda.empty_cache()\n",
    "    print(\"üßπ GPU cache cleared!\")\n",
    "    print(f\"Memory before training: {torch.cuda.memory_allocated() / 1e9:.2f} GB\")\n",
    "    \n",
    "    train_result = trainer.train()\n",
    "    \n",
    "    metrics = train_result.metrics\n",
    "    trainer.log_metrics(\"train\", metrics)\n",
    "    trainer.save_metrics(\"train\", metrics)\n",
    "    \n",
    "    print(\"\\n‚úÖ Training completed successfully!\")\n",
    "    print(\"\\nüìä Final Training Metrics:\")\n",
    "    for key, value in metrics.items():\n",
    "        print(f\"  ‚Ä¢ {key}: {value}\")\n",
    "except Exception as e:\n",
    "    print(f\"\\n‚ùå Training failed: {e}\")\n",
    "    print(\"Try reducing MAX_INPUT_LENGTH to 32 or restarting the kernel\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.status.busy": "2025-10-10T22:36:25.175459Z",
     "iopub.status.idle": "2025-10-10T22:36:25.175671Z",
     "shell.execute_reply": "2025-10-10T22:36:25.175578Z",
     "shell.execute_reply.started": "2025-10-10T22:36:25.175569Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "# NEW CELL: Test Sample Translation (Run after training)\n",
    "def translate_sample(text):\n",
    "    inputs = tokenizer(f\"{config.SOURCE_LANG} {config.TARGET_LANG} {text}\", return_tensors=\"pt\").to(device)\n",
    "    with torch.no_grad():\n",
    "        outputs = model.generate(**inputs, max_length=config.MAX_TARGET_LENGTH, num_beams=config.NUM_BEAMS)\n",
    "    pred = tokenizer.decode(outputs[0], skip_special_tokens=True)\n",
    "    # Normalize with processor\n",
    "    pred = processor.post_process_batch(pred, lang=config.TARGET_CODE)\n",
    "    return pred\n",
    "\n",
    "# Pick a test sample\n",
    "sample_en = raw_datasets['test'][0]['src']\n",
    "pred_ta = translate_sample(sample_en)\n",
    "ref_ta = raw_datasets['test'][0]['tgt']\n",
    "\n",
    "print(\"üìù Sample Translation Test:\")\n",
    "print(f\"English: {sample_en}\")\n",
    "print(f\"Predicted Tamil: {pred_ta}\")\n",
    "print(f\"Reference Tamil: {ref_ta}\")\n",
    "\n",
    "# Manual BLEU for this sample\n",
    "manual_bleu = bleu_metric.compute(predictions=[pred_ta], references=[[ref_ta]])\n",
    "print(f\"Manual BLEU for this sample: {manual_bleu['score']:.2f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "trusted": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "trusted": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "trusted": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "trusted": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kaggle": {
   "accelerator": "gpu",
   "dataSources": [
    {
     "datasetId": 1072502,
     "sourceId": 1805146,
     "sourceType": "datasetVersion"
    }
   ],
   "dockerImageVersionId": 31154,
   "isGpuEnabled": true,
   "isInternetEnabled": true,
   "language": "python",
   "sourceType": "notebook"
  },
  "kernelspec": {
   "display_name": "torchenv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
